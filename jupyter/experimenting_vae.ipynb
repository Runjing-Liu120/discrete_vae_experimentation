{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "import torchvision.datasets as dset\n",
    "import torchvision.transforms as transforms\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from torch.distributions import Normal\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "root = './data'\n",
    "if not os.path.exists(root):\n",
    "    os.mkdir(root)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "trans = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.5,), (1.0,))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train_set = dset.MNIST(root=root, train=True, transform=trans, download=False)\n",
    "test_set = dset.MNIST(root=root, train=False, transform=trans, download=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "batchsize = 100\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "                 dataset=train_set,\n",
    "                 batch_size=batchsize,\n",
    "                 shuffle=True)\n",
    "test_loader = torch.utils.data.DataLoader(\n",
    "                dataset=test_set,\n",
    "                batch_size=batchsize,\n",
    "                shuffle=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for batch_idx, d in enumerate(train_loader):\n",
    "    data = d\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class MLPEncoder(nn.Module):\n",
    "    def __init__(self, latent_dim = 5, \n",
    "                    n_classes = 10, \n",
    "                    slen = 28):\n",
    "        # the encoder returns the mean and variance of the latent parameters \n",
    "        # and the unconstrained symplex parametrization for the classes\n",
    "        \n",
    "        super(MLPEncoder, self).__init__()\n",
    "        \n",
    "        # image / model parameters\n",
    "        self.n_pixels = slen ** 2\n",
    "        self.latent_dim = latent_dim\n",
    "        self.n_classes = n_classes\n",
    "        self.slen = slen\n",
    "        \n",
    "        # define the linear layers        \n",
    "        self.fc1 = nn.Linear(self.n_pixels, 500)\n",
    "        self.fc2 = nn.Linear(500, self.n_pixels)\n",
    "        self.fc3 = nn.Linear(self.n_pixels, (n_classes - 1) + latent_dim * 2)\n",
    "        \n",
    "    \n",
    "        \n",
    "    def forward(self, image):\n",
    "        \n",
    "        # feed through neural network\n",
    "        z = image.view(-1, self.n_pixels)\n",
    "        \n",
    "        z = F.relu(self.fc1(z))\n",
    "        z = F.relu(self.fc2(z))\n",
    "        z = self.fc3(z)\n",
    "        \n",
    "        # get means, std, and class weights\n",
    "        indx1 = self.latent_dim\n",
    "        indx2 = 2 * self.latent_dim\n",
    "        indx3 = 2 * self.latent_dim + self.n_classes\n",
    "\n",
    "        latent_means = z[:, 0:indx1]\n",
    "        latent_std = torch.exp(z[:, indx1:indx2])\n",
    "        free_class_weights = z[:, indx2:indx3]\n",
    "\n",
    "\n",
    "        return latent_means, latent_std, free_class_weights\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "mlp_encoder = MLPEncoder()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "latent_means, latent_std, free_class_weights = mlp_encoder(data[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([100, 5])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "latent_means.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([100, 5])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "latent_std.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([100, 9])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "free_class_weights.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class MLPConditionalDecoder(nn.Module):\n",
    "    def __init__(self, latent_dim = 5, \n",
    "                        slen = 28):\n",
    "        \n",
    "        # This takes the latent parameters and returns the \n",
    "        # mean and variance for the image reconstruction\n",
    "        \n",
    "        super(MLPConditionalDecoder, self).__init__()\n",
    "        \n",
    "        # image/model parameters\n",
    "        self.n_pixels = slen ** 2\n",
    "        self.latent_dim = latent_dim\n",
    "        self.slen = slen\n",
    "        \n",
    "        self.fc1 = nn.Linear(latent_dim, self.n_pixels)\n",
    "        self.fc2 = nn.Linear(self.n_pixels, 500)\n",
    "        self.fc3 = nn.Linear(500, self.n_pixels * 2)\n",
    "        \n",
    "        \n",
    "    def forward(self, latent_params):\n",
    "        latent_params = latent_params.view(-1, self.latent_dim)\n",
    "        \n",
    "        z = F.relu(self.fc1(latent_params))\n",
    "        z = F.relu(self.fc2(z))\n",
    "        z = self.fc3(z)\n",
    "        \n",
    "        z = z.view(-1, 2, self.slen, self.slen)\n",
    "        \n",
    "        image_mean = z[:, 0, :, :]\n",
    "        image_std = torch.exp(z[:, 1, :, :])\n",
    "        \n",
    "        return image_mean, image_std\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "softmax = torch.nn.Softmax(dim=1)\n",
    "\n",
    "def get_symplex_from_reals(unconstrained_mat):\n",
    "    # first column is reference value \n",
    "    \n",
    "    aug_unconstrained_mat = torch.cat([torch.zeros((unconstrained_mat.shape[0], 1)), unconstrained_mat], 1)\n",
    "\n",
    "    return softmax(aug_unconstrained_mat)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_normal_loglik(x, mean, std, scale = False):\n",
    "    recon_losses = \\\n",
    "        Normal(mean, std).log_prob(x)\n",
    "\n",
    "    if scale:\n",
    "        factor = torch.prod(torch.Tensor([x.size()]))\n",
    "    else:\n",
    "        factor = 1.0\n",
    "\n",
    "    return (recon_losses / factor).view(x.size(0), -1).sum(1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_multinomial_entropy(z): \n",
    "    return (- z * torch.log(z)).sum(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class HandwritingVAE(nn.Module):\n",
    "\n",
    "    def __init__(self, latent_dim = 5, \n",
    "                    n_classes = 9, \n",
    "                    slen = 28):\n",
    "        \n",
    "        super(HandwritingVAE, self).__init__()\n",
    "                \n",
    "        self.encoder = MLPEncoder(latent_dim = latent_dim, \n",
    "                                    n_classes = n_classes, \n",
    "                                    slen = slen)\n",
    "        \n",
    "        # one decoder for each classes\n",
    "        self.decoder_list = [\n",
    "            MLPConditionalDecoder(latent_dim = latent_dim, slen = slen) for \n",
    "            k in range(n_classes)\n",
    "        ]\n",
    "        \n",
    "    def encoder_forward(self, image): \n",
    "        latent_means, latent_std, free_class_weights = mlp_encoder(image)\n",
    "        \n",
    "        class_weights = get_symplex_from_reals(free_class_weights)\n",
    "        \n",
    "        latent_samples = torch.randn(latent_means.shape) * latent_std + latent_means\n",
    "        \n",
    "        return latent_means, latent_std, latent_samples, class_weights\n",
    "        \n",
    "    def decoder_forward(self, latent_samples, z): \n",
    "        assert z <= len(self.decoder_list)\n",
    "        \n",
    "        image_mean, image_std = self.decoder_list[z](latent_samples)\n",
    "                \n",
    "        return image_mean, image_std\n",
    "    \n",
    "    def loss(self, image): \n",
    "        \n",
    "        latent_means, latent_std, latent_samples, class_weights = \\\n",
    "            self.encoder_forward(image)\n",
    "        \n",
    "        # likelihood term\n",
    "        loss = torch.zeros(image.shape[0])\n",
    "        for z in range(self.encoder.n_classes): \n",
    "            image_mu, image_std = self.decoder_forward(latent_samples, z)\n",
    "            \n",
    "            normal_loglik_z = get_normal_loglik(image, image_mu, image_std, scale = False)\n",
    "            \n",
    "            loss = - class_weights[:, z] * normal_loglik_z\n",
    "        \n",
    "        # kl term for latent parameters\n",
    "        # (assuming standard normal)\n",
    "        kl_q_latent = - 0.5 * torch.sum(1 + torch.log(latent_std**2) - latent_means**2 -  latent_std**2, dim = 1)\n",
    "        \n",
    "        # entropy term for class weights\n",
    "        # (assuming uniform prior)\n",
    "        kl_q_z = get_multinomial_entropy(class_weights)\n",
    "        \n",
    "        print(loss)\n",
    "        print(kl_q_latent)\n",
    "        print(kl_q_z)\n",
    "        \n",
    "        loss -= (kl_q_latent + kl_q_z)\n",
    "        \n",
    "        return loss.sum()\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "vae = HandwritingVAE()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "latent_means, latent_std, latent_samples, class_weights = vae.encoder_forward(data[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "image_mean, image_std = vae.decoder_list[0](latent_samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([100, 28, 28])"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "image_mean.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([100, 28, 28])"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "image_std.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "image_mean, iimage_std = vae.decoder_forward(latent_samples, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 8543.1016,  8332.4219,  7947.0059,  8550.3154,  8093.6519,\n",
      "         7876.5171,  7882.1270,  8161.7554,  7927.6196,  8117.5054,\n",
      "         8203.8857,  8246.5693,  8021.0996,  7969.1104,  8119.1792,\n",
      "         7718.4849,  7838.4517,  7801.1582,  7971.8354,  8094.5513,\n",
      "         8239.6621,  7892.6416,  8065.9780,  7942.0918,  8415.4297,\n",
      "         8123.5371,  8102.1958,  8143.0269,  8367.7871,  8395.6533,\n",
      "         7835.7817,  8177.0493,  8048.0879,  8163.0869,  8145.0347,\n",
      "         7942.4150,  7751.6528,  8121.9170,  7870.2041,  7841.1924,\n",
      "         8100.6704,  7836.3667,  7584.6279,  7873.9214,  8226.1582,\n",
      "         8183.6055,  8090.4375,  7856.8950,  8214.0107,  7865.9697,\n",
      "         8387.6338,  8634.8652,  8095.6836,  8189.0356,  8532.8340,\n",
      "         8109.4482,  8403.6201,  7909.1514,  8042.9702,  8172.4751,\n",
      "         8201.2979,  7880.9258,  8323.4893,  8297.9150,  7945.9302,\n",
      "         8130.6460,  8246.4238,  8414.6416,  7860.6675,  7972.5337,\n",
      "         7843.1748,  8187.8379,  8117.8960,  7862.7900,  8069.4238,\n",
      "         7985.6880,  8718.6191,  8112.8799,  7894.4824,  8100.6069,\n",
      "         7771.9624,  8003.0210,  7991.2261,  8299.7988,  8189.2529,\n",
      "         8294.0664,  7703.7939,  8046.3105,  8016.1245,  8125.8325,\n",
      "         8433.4863,  7920.3315,  7879.5679,  8377.1270,  8030.3599,\n",
      "         8039.0034,  7930.5200,  8232.5391,  8150.8940,  7975.8345])\n",
      "tensor(1.00000e-02 *\n",
      "       [ 2.2291,  1.6426,  3.1664,  2.5243,  2.2908,  1.7270,  2.2423,\n",
      "         2.4078,  1.3823,  1.8328,  1.9629,  3.4153,  2.2533,  1.9165,\n",
      "         3.0447,  2.0506,  1.8683,  2.0327,  2.4341,  1.8500,  1.4960,\n",
      "         0.8981,  1.5789,  2.7022,  1.7989,  1.6962,  1.7016,  3.8649,\n",
      "         1.8276,  2.9041,  2.1132,  1.8667,  1.4699,  2.1237,  3.0118,\n",
      "         1.7939,  4.0050,  2.3973,  1.4583,  2.5275,  1.3849,  1.4914,\n",
      "         2.0516,  2.0779,  3.3582,  1.6637,  1.2395,  1.5291,  1.9382,\n",
      "         1.7524,  1.9617,  1.5684,  2.9065,  2.3725,  2.0237,  1.7519,\n",
      "         2.0338,  1.6704,  1.8903,  1.2789,  2.4687,  3.2649,  1.7884,\n",
      "         2.5289,  2.0241,  2.4007,  1.5251,  2.0635,  1.5117,  2.7503,\n",
      "         1.1731,  2.3364,  2.0202,  1.4792,  2.8992,  2.7293,  2.0541,\n",
      "         1.5623,  1.8857,  2.4436,  3.2492,  1.7993,  2.2764,  2.1680,\n",
      "         1.8839,  1.5633,  2.3686,  2.3556,  2.0188,  2.6993,  1.8686,\n",
      "         2.9930,  2.0307,  2.0323,  1.9577,  3.0113,  2.6904,  1.5162,\n",
      "         1.8309,  1.7447])\n",
      "tensor([ 2.3015,  2.3017,  2.3016,  2.3012,  2.3017,  2.3012,  2.3014,\n",
      "         2.3013,  2.3013,  2.3015,  2.3010,  2.3012,  2.3005,  2.3014,\n",
      "         2.3015,  2.3014,  2.3008,  2.3009,  2.3014,  2.3007,  2.3012,\n",
      "         2.3016,  2.3013,  2.3015,  2.3013,  2.3015,  2.3006,  2.3018,\n",
      "         2.3014,  2.3016,  2.3015,  2.3012,  2.3014,  2.3013,  2.3014,\n",
      "         2.3010,  2.3010,  2.3018,  2.3004,  2.3014,  2.3016,  2.3011,\n",
      "         2.3010,  2.3013,  2.3015,  2.3014,  2.3013,  2.3004,  2.3018,\n",
      "         2.3013,  2.3015,  2.3012,  2.3013,  2.3016,  2.3012,  2.3018,\n",
      "         2.3018,  2.3016,  2.3012,  2.3015,  2.3007,  2.3011,  2.3015,\n",
      "         2.3016,  2.3013,  2.3011,  2.3013,  2.3013,  2.3021,  2.3014,\n",
      "         2.3012,  2.3015,  2.3015,  2.3014,  2.3014,  2.3017,  2.3011,\n",
      "         2.3011,  2.3008,  2.3016,  2.3011,  2.3012,  2.3016,  2.3021,\n",
      "         2.3020,  2.3017,  2.3006,  2.3014,  2.3012,  2.3016,  2.3017,\n",
      "         2.3016,  2.3012,  2.3015,  2.3015,  2.3017,  2.3013,  2.3015,\n",
      "         2.3016,  2.3015])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor(1.00000e+05 *\n",
       "       8.0866)"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vae.loss(data[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (pytorch_04)",
   "language": "python",
   "name": "pytorch_update"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
