{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline  \n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "\n",
    "import torch.optim as optim\n",
    "\n",
    "import sys\n",
    "sys.path.insert(0, '../libraries/')\n",
    "\n",
    "import mnist_data_lib\n",
    "import mnist_vae_lib\n",
    "import common_utils"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "seed = 9033"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data\n",
      "Done.\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(seed)\n",
    "_ = torch.manual_seed(seed)\n",
    "\n",
    "# LOAD DATA\n",
    "print('Loading data')\n",
    "train_set_labeled, train_set_unlabeled, test_set = \\\n",
    "    mnist_data_lib.get_mnist_dataset_semisupervised(propn_sample = 1.0,\n",
    "                                                    propn_labeled = 0.1)\n",
    "\n",
    "train_loader_labeled = torch.utils.data.DataLoader(\n",
    "                 dataset=train_set_labeled,\n",
    "                 batch_size=20,\n",
    "                 shuffle=True)\n",
    "\n",
    "train_loader_unlabeled = torch.utils.data.DataLoader(\n",
    "                 dataset=train_set_unlabeled,\n",
    "                 batch_size=100,\n",
    "                 shuffle=True)\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(\n",
    "                dataset=test_set,\n",
    "                batch_size=100,\n",
    "                shuffle=False)\n",
    "\n",
    "for batch_idx, d in enumerate(train_loader_labeled):\n",
    "    data_labeled = d\n",
    "    break\n",
    "\n",
    "for batch_idx, d in enumerate(train_loader_unlabeled):\n",
    "    data_unlabeled = d\n",
    "    break\n",
    "    \n",
    "for batch_idx, d in enumerate(test_loader):\n",
    "    test_data = d\n",
    "    break\n",
    "\n",
    "\n",
    "print('Done.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7fe91f7eb2e8>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 288x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.matshow(data_labeled['image'][0, :, :])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "slen = data_labeled['image'].shape[-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define VAE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "latent_dim = 5\n",
    "n_classes = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "vae = mnist_vae_lib.HandwritingVAE(latent_dim = latent_dim, \n",
    "                                    n_classes = n_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Error(s) in loading state_dict for MLPEncoder:\n\tWhile copying the parameter named \"fc1.weight\", whose dimensions in the model are torch.Size([128, 794]) and whose dimensions in the checkpoint are torch.Size([128, 784]).",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-642bb381a291>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mclassifier_file\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'../mnist_vae_results_aws/mnist_vae3_semisupervised_classifier_epoch30'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m vae.encoder.load_state_dict(torch.load(enc_file,\n\u001b[0;32m----> 7\u001b[0;31m                                map_location=lambda storage, loc: storage))\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m vae.decoder.load_state_dict(torch.load(dec_file,\n",
      "\u001b[0;32m~/anaconda3/envs/pytorch_update/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36mload_state_dict\u001b[0;34m(self, state_dict, strict)\u001b[0m\n\u001b[1;32m    719\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merror_msgs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    720\u001b[0m             raise RuntimeError('Error(s) in loading state_dict for {}:\\n\\t{}'.format(\n\u001b[0;32m--> 721\u001b[0;31m                                self.__class__.__name__, \"\\n\\t\".join(error_msgs)))\n\u001b[0m\u001b[1;32m    722\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    723\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mparameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Error(s) in loading state_dict for MLPEncoder:\n\tWhile copying the parameter named \"fc1.weight\", whose dimensions in the model are torch.Size([128, 794]) and whose dimensions in the checkpoint are torch.Size([128, 784])."
     ]
    }
   ],
   "source": [
    "# Load results\n",
    "\n",
    "enc_file = \"../mnist_vae_results_aws/mnist_vae3_semisupervised_enc_epoch30\"\n",
    "dec_file = '../mnist_vae_results_aws/mnist_vae3_semisupervised_dec_epoch30'\n",
    "classifier_file = '../mnist_vae_results_aws/mnist_vae3_semisupervised_classifier_epoch30'\n",
    "vae.encoder.load_state_dict(torch.load(enc_file,\n",
    "                               map_location=lambda storage, loc: storage))\n",
    "\n",
    "vae.decoder.load_state_dict(torch.load(dec_file,\n",
    "                               map_location=lambda storage, loc: storage))\n",
    "\n",
    "vae.classifier.load_state_dict(torch.load(classifier_file,\n",
    "                               map_location=lambda storage, loc: storage))\n",
    "\n",
    "\n",
    "vae.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(vae.parameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "loss_array = np.loadtxt('../mnist_vae_results_aws/mnist_vae2_semisupervisedloss_array.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.plot(loss_array[0, :], loss_array[1, :], '+')\n",
    "plt.plot(loss_array[0, :], loss_array[2, :], '+')\n",
    "plt.xlabel('epoch')\n",
    "plt.ylabel('loss')\n",
    "plt.legend(('train loss', 'test_loss'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "vae.loss(data_unlabeled['image'])[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "vae.loss(test_data['image'])[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# check a few train images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "latent_means, latent_std, latent_samples, class_weights = \\\n",
    "    vae.encoder_forward(data_unlabeled['image'])\n",
    "\n",
    "z_ind = torch.argmax(class_weights, dim = 1)\n",
    "    \n",
    "image_mu, image_std = vae.decoder_forward(latent_means, z_ind)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for i in range(10): \n",
    "    plt.matshow(image_mu[i, :, :].detach())\n",
    "    plt.title('true class label: {} \\n predicted class label: {}\\n'.format(data_unlabeled['label'][i], z_ind[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# check a few test images "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "latent_means, latent_std, latent_samples, class_weights = \\\n",
    "    vae.encoder_forward(test_data['image'])\n",
    "\n",
    "z_ind = torch.argmax(class_weights, dim = 1)\n",
    "\n",
    "image_mu, image_std = vae.decoder_forward(latent_means, z_ind)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for i in range(10): \n",
    "    plt.matshow(image_mu[i, :, :].detach())\n",
    "    plt.title('true class label: {} \\n predicted class label: {}\\n'.format(test_data['label'][i], z_ind[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "_, _, _, class_weights_unlab = \\\n",
    "    vae.encoder_forward(data_unlabeled['image'])\n",
    "\n",
    "_, _, _, class_weights_lab = \\\n",
    "    vae.encoder_forward(data_labeled['image'])\n",
    "\n",
    "_, _, _, class_weights_test = \\\n",
    "    vae.encoder_forward(test_data['image'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "vae.get_class_label_cross_entropy(class_weights_unlab, data_unlabeled['label']) / len(data_unlabeled['label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "vae.get_class_label_cross_entropy(class_weights_lab, data_labeled['label']) / len(data_labeled['label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "vae.get_class_label_cross_entropy(class_weights_test, test_data['label'])  / len(test_data['label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class_weights_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class_weights_lab.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.matshow(class_weights_lab.detach())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "common_utils.get_one_hot_encoding_from_int(test_data['label'], vae.encoder.n_classes).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (pytorch_04)",
   "language": "python",
   "name": "pytorch_update"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
