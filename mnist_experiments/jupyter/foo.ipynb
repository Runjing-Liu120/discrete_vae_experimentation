{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "import torchvision.datasets as dset\n",
    "import torchvision.transforms as transforms\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from torch.distributions import Normal\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "root = './data'\n",
    "if not os.path.exists(root):\n",
    "    os.mkdir(root)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "trans = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.5,), (1.0,))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Dataset not found. You can use download=True to download it",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-d9d8f81d3923>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrain_set\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mMNIST\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mroot\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mroot\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtransform\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrans\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdownload\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mtest_set\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mMNIST\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mroot\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mroot\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtransform\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrans\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdownload\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/pytorch_update/lib/python3.6/site-packages/torchvision-0.2.1-py3.6.egg/torchvision/datasets/mnist.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, root, train, transform, target_transform, download)\u001b[0m\n\u001b[1;32m     48\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_check_exists\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m             raise RuntimeError('Dataset not found.' +\n\u001b[0;32m---> 50\u001b[0;31m                                ' You can use download=True to download it')\n\u001b[0m\u001b[1;32m     51\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Dataset not found. You can use download=True to download it"
     ]
    }
   ],
   "source": [
    "train_set = dset.MNIST(root=root, train=True, transform=trans, download=False)\n",
    "test_set = dset.MNIST(root=root, train=False, transform=trans, download=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'train_set' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-ff2bde718c7b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m train_loader = torch.utils.data.DataLoader(\n\u001b[0;32m----> 4\u001b[0;31m                  \u001b[0mdataset\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrain_set\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m                  \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatchsize\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m                  shuffle=True)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'train_set' is not defined"
     ]
    }
   ],
   "source": [
    "batchsize = 100\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "                 dataset=train_set,\n",
    "                 batch_size=batchsize,\n",
    "                 shuffle=True)\n",
    "test_loader = torch.utils.data.DataLoader(\n",
    "                dataset=test_set,\n",
    "                batch_size=batchsize,\n",
    "                shuffle=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'train_loader' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-3daf685218ff>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0mbatch_idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0md\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_loader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m     \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0md\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'train_loader' is not defined"
     ]
    }
   ],
   "source": [
    "for batch_idx, d in enumerate(train_loader):\n",
    "    data = d\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class MLPEncoder(nn.Module):\n",
    "    def __init__(self, latent_dim = 5, \n",
    "                    n_classes = 10, \n",
    "                    slen = 28):\n",
    "        # the encoder returns the mean and variance of the latent parameters \n",
    "        # and the unconstrained symplex parametrization for the classes\n",
    "        \n",
    "        super(MLPEncoder, self).__init__()\n",
    "        \n",
    "        # image / model parameters\n",
    "        self.n_pixels = slen ** 2\n",
    "        self.latent_dim = latent_dim\n",
    "        self.n_classes = n_classes\n",
    "        self.slen = slen\n",
    "        \n",
    "        # define the linear layers        \n",
    "        self.fc1 = nn.Linear(self.n_pixels, 500)\n",
    "        self.fc2 = nn.Linear(500, self.n_pixels)\n",
    "        self.fc3 = nn.Linear(self.n_pixels, (n_classes - 1) + latent_dim * 2)\n",
    "        \n",
    "    \n",
    "        \n",
    "    def forward(self, image):\n",
    "        \n",
    "        # feed through neural network\n",
    "        z = image.view(-1, self.n_pixels)\n",
    "        \n",
    "        z = F.relu(self.fc1(z))\n",
    "        z = F.relu(self.fc2(z))\n",
    "        z = self.fc3(z)\n",
    "        \n",
    "        # get means, std, and class weights\n",
    "        indx1 = self.latent_dim\n",
    "        indx2 = 2 * self.latent_dim\n",
    "        indx3 = 2 * self.latent_dim + self.n_classes\n",
    "\n",
    "        latent_means = z[:, 0:indx1]\n",
    "        latent_std = torch.exp(z[:, indx1:indx2])\n",
    "        free_class_weights = z[:, indx2:indx3]\n",
    "\n",
    "\n",
    "        return latent_means, latent_std, free_class_weights\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "mlp_encoder = MLPEncoder()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "latent_means, latent_std, free_class_weights = mlp_encoder(data[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([100, 5])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "latent_means.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([100, 5])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "latent_std.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([100, 9])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "free_class_weights.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class MLPConditionalDecoder(nn.Module):\n",
    "    def __init__(self, latent_dim = 5, \n",
    "                        slen = 28):\n",
    "        \n",
    "        # This takes the latent parameters and returns the \n",
    "        # mean and variance for the image reconstruction\n",
    "        \n",
    "        super(MLPConditionalDecoder, self).__init__()\n",
    "        \n",
    "        # image/model parameters\n",
    "        self.n_pixels = slen ** 2\n",
    "        self.latent_dim = latent_dim\n",
    "        self.slen = slen\n",
    "        \n",
    "        self.fc1 = nn.Linear(latent_dim, self.n_pixels)\n",
    "        self.fc2 = nn.Linear(self.n_pixels, 500)\n",
    "        self.fc3 = nn.Linear(500, self.n_pixels * 2)\n",
    "        \n",
    "        \n",
    "    def forward(self, latent_params):\n",
    "        latent_params = latent_params.view(-1, self.latent_dim)\n",
    "        \n",
    "        z = F.relu(self.fc1(latent_params))\n",
    "        z = F.relu(self.fc2(z))\n",
    "        z = self.fc3(z)\n",
    "        \n",
    "        z = z.view(-1, 2, self.slen, self.slen)\n",
    "        \n",
    "        image_mean = z[:, 0, :, :]\n",
    "        image_std = torch.exp(z[:, 1, :, :])\n",
    "        \n",
    "        return image_mean, image_std\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "softmax = torch.nn.Softmax(dim=1)\n",
    "\n",
    "def get_symplex_from_reals(unconstrained_mat):\n",
    "    # first column is reference value \n",
    "    \n",
    "    aug_unconstrained_mat = torch.cat([torch.zeros((unconstrained_mat.shape[0], 1)), unconstrained_mat], 1)\n",
    "\n",
    "    return softmax(aug_unconstrained_mat)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_normal_loglik(x, mean, std, scale = False):\n",
    "    recon_losses = \\\n",
    "        Normal(mean, std).log_prob(x)\n",
    "\n",
    "    if scale:\n",
    "        factor = torch.prod(torch.Tensor([x.size()]))\n",
    "    else:\n",
    "        factor = 1.0\n",
    "\n",
    "    return (recon_losses / factor).view(x.size(0), -1).sum(1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_multinomial_entropy(z): \n",
    "    return (- z * torch.log(z)).sum(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_kl_q_standard_normal(mu, sigma): \n",
    "    return - 0.5 * torch.sum(-1 - torch.log(sigma**2) + mu**2 + sigma**2, dim = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class HandwritingVAE(nn.Module):\n",
    "\n",
    "    def __init__(self, latent_dim = 5, \n",
    "                    n_classes = 9, \n",
    "                    slen = 28):\n",
    "        \n",
    "        super(HandwritingVAE, self).__init__()\n",
    "                \n",
    "        self.encoder = MLPEncoder(latent_dim = latent_dim, \n",
    "                                    n_classes = n_classes, \n",
    "                                    slen = slen)\n",
    "        \n",
    "        # one decoder for each classes\n",
    "        self.decoder_list = [\n",
    "            MLPConditionalDecoder(latent_dim = latent_dim, slen = slen) for \n",
    "            k in range(n_classes)\n",
    "        ]\n",
    "        \n",
    "    def encoder_forward(self, image): \n",
    "        latent_means, latent_std, free_class_weights = self.encoder(image)\n",
    "        \n",
    "        class_weights = get_symplex_from_reals(free_class_weights)\n",
    "        \n",
    "        latent_samples = torch.randn(latent_means.shape) * latent_std + latent_means\n",
    "        \n",
    "        return latent_means, latent_std, latent_samples, class_weights\n",
    "        \n",
    "    def decoder_forward(self, latent_samples, z): \n",
    "        assert z <= len(self.decoder_list)\n",
    "        \n",
    "        image_mean, image_std = self.decoder_list[z](latent_samples)\n",
    "                \n",
    "        return image_mean, image_std\n",
    "    \n",
    "    def loss(self, image): \n",
    "        \n",
    "        latent_means, latent_std, latent_samples, class_weights = \\\n",
    "            self.encoder_forward(image)\n",
    "        \n",
    "        # likelihood term\n",
    "        loss = 0.0\n",
    "        for z in range(self.encoder.n_classes): \n",
    "            image_mu, image_std = self.decoder_forward(latent_samples, z)\n",
    "            \n",
    "            normal_loglik_z = get_normal_loglik(image, image_mu, image_std, scale = False)\n",
    "            \n",
    "            loss = - (class_weights[:, z] * normal_loglik_z).sum()\n",
    "        \n",
    "        # kl term for latent parameters\n",
    "        # (assuming standard normal prior)\n",
    "        kl_q_latent = get_kl_q_standard_normal(latent_means, latent_std).sum()\n",
    "        \n",
    "        # entropy term for class weights\n",
    "        # (assuming uniform prior)\n",
    "        kl_q_z = get_multinomial_entropy(class_weights).sum()\n",
    "        \n",
    "        loss -= (kl_q_latent + kl_q_z)\n",
    "        \n",
    "        return loss / image.size()[0]\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "vae = HandwritingVAE()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "latent_means, latent_std, latent_samples, class_weights = vae.encoder_forward(data[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "image_mean, image_std = vae.decoder_list[0](latent_samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([100, 28, 28])"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "image_mean.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([100, 28, 28])"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "image_std.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "image_mean, image_std = vae.decoder_forward(latent_samples, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([100])"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_normal_loglik(data[0], image_mean, image_std, scale = False).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 2.1966,  2.1961,  2.1963,  2.1968,  2.1963,  2.1962,  2.1963,\n",
       "         2.1965,  2.1967,  2.1959,  2.1961,  2.1964,  2.1967,  2.1967,\n",
       "         2.1964,  2.1965,  2.1967,  2.1967,  2.1969,  2.1961,  2.1966,\n",
       "         2.1962,  2.1967,  2.1958,  2.1965,  2.1963,  2.1967,  2.1965,\n",
       "         2.1967,  2.1962,  2.1967,  2.1960,  2.1965,  2.1964,  2.1967,\n",
       "         2.1964,  2.1965,  2.1960,  2.1963,  2.1965,  2.1966,  2.1968,\n",
       "         2.1962,  2.1962,  2.1966,  2.1963,  2.1965,  2.1966,  2.1966,\n",
       "         2.1967,  2.1969,  2.1964,  2.1964,  2.1969,  2.1967,  2.1960,\n",
       "         2.1967,  2.1964,  2.1964,  2.1964,  2.1965,  2.1960,  2.1967,\n",
       "         2.1967,  2.1967,  2.1965,  2.1962,  2.1966,  2.1962,  2.1958,\n",
       "         2.1965,  2.1967,  2.1966,  2.1965,  2.1963,  2.1969,  2.1965,\n",
       "         2.1962,  2.1963,  2.1967,  2.1963,  2.1963,  2.1967,  2.1962,\n",
       "         2.1964,  2.1966,  2.1967,  2.1961,  2.1962,  2.1965,  2.1964,\n",
       "         2.1961,  2.1968,  2.1964,  2.1965,  2.1968,  2.1967,  2.1965,\n",
       "         2.1964,  2.1966])"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_multinomial_entropy(class_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1.00000e-02 *\n",
       "       [-4.2507, -4.4494, -3.7238, -3.4022, -4.5963, -3.4034, -3.5204,\n",
       "        -3.9656, -3.7141, -3.3468, -3.3739, -3.4204, -5.4215, -4.4266,\n",
       "        -5.0040, -2.1904, -4.7955, -3.3199, -4.1232, -2.9153, -5.2886,\n",
       "        -3.7407, -3.3638, -3.1879, -4.1887, -4.6355, -3.8229, -3.3988,\n",
       "        -5.2633, -3.8584, -4.6044, -3.9608, -4.6817, -4.4650, -4.7966,\n",
       "        -4.1885, -4.9280, -4.5644, -3.1350, -3.9155, -4.6217, -3.4721,\n",
       "        -3.2481, -4.1628, -3.4657, -3.8335, -3.7208, -3.5248, -4.4941,\n",
       "        -3.5619, -3.6239, -3.1549, -3.4464, -3.4044, -4.2088, -4.5232,\n",
       "        -5.8070, -4.3638, -4.4337, -3.4706, -3.4942, -2.0047, -4.2396,\n",
       "        -4.1518, -4.4882, -5.6319, -3.6247, -3.5661, -4.1360, -2.4366,\n",
       "        -3.8619, -3.9487, -5.0312, -3.7137, -5.0772, -4.6305, -2.6672,\n",
       "        -3.2963, -4.0577, -2.9271, -3.5864, -4.4544, -3.6276, -4.2248,\n",
       "        -5.0598, -3.8440, -4.8037, -3.7238, -3.5523, -3.3934, -4.1262,\n",
       "        -3.8073, -3.5899, -3.3761, -4.8962, -4.7021, -4.4049, -4.0135,\n",
       "        -4.9130, -3.9261])"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_kl_q_standard_normal(latent_means, latent_std)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(9018.0166)"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vae.loss(data[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (pytorch_04)",
   "language": "python",
   "name": "pytorch_update"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
